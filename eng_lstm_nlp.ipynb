{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1LmcCjtGQCrWsx6TvF6uJMxzkfW9jDibN",
      "authorship_tag": "ABX9TyNBs9xeenBKlqDvCT6K+NkG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluecityisu/class2022/blob/main/eng_lstm_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V7LdQRTM-Zq"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "gFlh4NayNnjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QXl0w9mMNy1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "table=str.maketrans('', '', string.punctuation)"
      ],
      "metadata": {
        "id": "v8qh444GN4Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = [\"I\",\"by\",\"i\"]"
      ],
      "metadata": {
        "id": "p2b4VB8oN-QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for chinese only\n",
        "\"\"\"\n",
        "def getch(s):\n",
        "  str33=[]\n",
        "  res = re.compile(r\"([\\u4e00-\\u9fa5])\")\n",
        "  ret = res.split(s)\n",
        "  for ch in ret:\n",
        "    if ch!='' and ch!=' ':\n",
        "      str33.append(ch)\n",
        "  str33=np.array(str33)\n",
        "  return str33\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bfKacLwTOIT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[]\n",
        "labels=[]\n",
        "with open('/content/eng3_j.csv', encoding='UTF-8') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=\",\")\n",
        "  for row in reader:\n",
        "    labels.append(row[0])\n",
        "    sentence = row[1].lower()\n",
        "    sentence = sentence.replace(\"_\", \" _ \")\n",
        "    sentence = sentence.replace(\"/\", \" / \")\n",
        "    soup = BeautifulSoup(sentence)\n",
        "    sentence = soup.get_text()\n",
        "\n",
        "    words = sentence.split()\n",
        "    #words=getch(sentence)\n",
        "    filtered_sentence = \"\"\n",
        "    for word in words:\n",
        "        word = word.translate(table)\n",
        "        if word not in stopwords:\n",
        "            filtered_sentence = filtered_sentence + word + \" \"\n",
        "    sentences.append(filtered_sentence)\n",
        "\n",
        "print(len(labels))\n",
        "print(len(sentences))\n",
        "print(sentences[5])"
      ],
      "metadata": {
        "id": "5ljyFSKIOX0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[9]"
      ],
      "metadata": {
        "id": "3tYoZ4fNQikw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 200\n",
        "embedding_dim = 6\n",
        "max_length = 50\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"\""
      ],
      "metadata": {
        "id": "Bp3-fW0aQmIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "sent2=[]\n",
        "for nx in sentences:\n",
        "  nn2=getch(nx)\n",
        "  nn2=nn2.flatten()\n",
        "  sent2.append(nn2)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "27WimtjKQrVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sent2[0]"
      ],
      "metadata": {
        "id": "y7xeWwauQxt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sent2=np.array(sentences)\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "sWzIgesCQ11e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "id": "WOl0uNTNQ6Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "ojGybg4HQ-We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sentences)\n",
        "sentences=pad_sequences(sentences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "id": "A1Lep1x5RBVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels=[x.replace(\" \", \"\") for x in labels]"
      ],
      "metadata": {
        "id": "aAkFKU8ORZYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels[0]=0"
      ],
      "metadata": {
        "id": "vhqbGZL7RbuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(training_labels)):\n",
        "  if training_labels[x]=='0':\n",
        "    training_labels[x]=0\n",
        "  elif training_labels[x]=='1':\n",
        "    training_labels[x]=1\n",
        "  elif training_labels[x]=='2':\n",
        "    training_labels[x]=2\n",
        "  else:\n",
        "    training_labels[x]=int(training_labels[x])\n"
      ],
      "metadata": {
        "id": "E40gokfLRe1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels=np.array(training_labels)\n"
      ],
      "metadata": {
        "id": "Q5xQDb-zRjsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_labels)"
      ],
      "metadata": {
        "id": "wTJxL14-Rk_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ys = tf.keras.utils.to_categorical(training_labels, num_classes=5)"
      ],
      "metadata": {
        "id": "r1MzfaudRsoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ys))"
      ],
      "metadata": {
        "id": "GRAhATyBRz6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(LSTM(max_length-1, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(LSTM(max_length-1)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "CByG-JxOR3Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 400\n",
        "history = model.fit(sentences, ys, epochs=num_epochs, verbose=2)"
      ],
      "metadata": {
        "id": "oveNNE3jSEfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cate=[\"Where are you shipping for?\",\"$100\",\"$30\",\"$70\",\"45\"]\n",
        "mylist=[]\n",
        "nn3=[]\n",
        "str3 = [\"I want to use air freight\",\"air freight taiwan\",\"sea freight taiwan\",\"sea freight\"]\n",
        "str3=np.array(str3)\n",
        "for xx,yy in enumerate(str3):\n",
        "  str3[xx]=yy.replace(\"?\",\"\")\n",
        "  str3[xx]=yy.replace(\"I\",\"\")\n",
        "\n",
        "str3 = tokenizer.texts_to_sequences(str3)\n",
        "#str3=np.array(str3)\n",
        "mylist = pad_sequences(str3, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "\n",
        "predict=model.predict(mylist)\n",
        "print(np.round(predict,2))\n",
        "print(np.argmax(predict,axis=1))\n",
        "result=[cate[x] for x in np.argmax(predict,axis=1)]\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "70CMgYCmSW7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preds(x):\n",
        "  mylist=[]\n",
        "  str3=x.replace(\"?\",\"\")\n",
        "  str3=np.array(str3)\n",
        "  str3 = np.expand_dims(str3, axis=0)\n",
        "  str3 = tokenizer.texts_to_sequences(str3)\n",
        "\n",
        "  mylist = pad_sequences(str3, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "  predict=model.predict(mylist)\n",
        "\n",
        "  pred_index=np.argmax(predict,axis=1)\n",
        "  #print(pred_index)\n",
        "  return cate[pred_index[0]]"
      ],
      "metadata": {
        "id": "r4aYQg8wSgU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1=input(\"air or sea?\")\n",
        "a1=preds(q1)\n",
        "\n",
        "q2=input(a1)\n",
        "\n",
        "a2=preds(q1 + \" \" + q2)\n",
        "print(a2)"
      ],
      "metadata": {
        "id": "0xD2GbZWSlO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/class/tmp/rps.h5\")"
      ],
      "metadata": {
        "id": "NxeVuxpHS9Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model you save before\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/class/tmp/rps.h5')"
      ],
      "metadata": {
        "id": "GQVDd4u7Q7yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflowjs as tfjs"
      ],
      "metadata": {
        "id": "tVtZDhaRTX_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfjs.converters.save_keras_model(model, 'models')"
      ],
      "metadata": {
        "id": "dtoV-rnGTcKG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}