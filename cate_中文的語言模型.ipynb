{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1tv_NOHtPL9Cq_zwavCWB2ARXOb7giRMa",
      "authorship_tag": "ABX9TyOoEvFH/ukhyNGfGffcElqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluecityisu/class2022/blob/main/cate_%E4%B8%AD%E6%96%87%E7%9A%84%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "YJ5UERt9VzAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "a3eQ9QgScLbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JuoV3TgjbsRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "table=str.maketrans('', '', string.punctuation)"
      ],
      "metadata": {
        "id": "C_-KU2SSV-SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = [\"我\",\"了\"]"
      ],
      "metadata": {
        "id": "s3f2C6DoSxuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getch(s):\n",
        "  str33=[]\n",
        "  res = re.compile(r\"([\\u4e00-\\u9fa5])\")\n",
        "  ret = res.split(s)\n",
        "  for ch in ret:\n",
        "    if ch!='' and ch!=' ':\n",
        "      str33.append(ch)\n",
        "  str33=np.array(str33)\n",
        "  return str33"
      ],
      "metadata": {
        "id": "B1a5yvTgVVEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[]\n",
        "labels=[]\n",
        "with open('/content/drive/MyDrive/TensorFlow_Book/Deep_learning_coder/ch3.csv', encoding='UTF-8') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=\",\")\n",
        "  for row in reader:\n",
        "    labels.append(row[0])\n",
        "    sentence = row[1].lower()\n",
        "    sentence = sentence.replace(\"，\", \",\")\n",
        "    sentence = sentence.replace(\"?\", \"?\")\n",
        "    sentence = sentence.replace(\"。\", \".\")\n",
        "    sentence = sentence.replace(\"/\", \" / \")\n",
        "    soup = BeautifulSoup(sentence)\n",
        "    sentence = soup.get_text()\n",
        "\n",
        "    #words = sentence.split()\n",
        "    words=getch(sentence)\n",
        "    filtered_sentence = \"\"\n",
        "    for word in words:\n",
        "        word = word.translate(table)\n",
        "        if word not in stopwords:\n",
        "            filtered_sentence = filtered_sentence + word\n",
        "    sentences.append(filtered_sentence)\n",
        "\n",
        "print(len(labels))\n",
        "print(len(sentences))\n",
        "print(sentences[4])"
      ],
      "metadata": {
        "id": "mKwi2IJhSLUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "id": "93Gofv-xyIfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 200\n",
        "embedding_dim = 6\n",
        "max_length = 10\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "metadata": {
        "id": "vd_mMyY-V0VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent2=[]\n",
        "for nx in sentences:\n",
        "  nn2=getch(nx)\n",
        "  nn2=nn2.flatten()\n",
        "  sent2.append(nn2)"
      ],
      "metadata": {
        "id": "37fPYJMXWqAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent2[0]"
      ],
      "metadata": {
        "id": "Lm3bAiYvY30c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent2=np.array(sent2)\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "for xxx in sent2:\n",
        "  tokenizer.fit_on_texts(xxx)\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "X_j5aSkBVtjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "id": "aAWeekR8WII1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "yBGfL0-kbz9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mylist=[]\n",
        "\n",
        "for nn in sentences:\n",
        "  nn2=getch(nn)\n",
        "  nn2 = tokenizer.texts_to_sequences(nn2)\n",
        "  nn2=np.array(nn2)\n",
        "  #padd = pad_sequences(str3, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "  nn2=nn2.flatten()\n",
        "  mylist.append(nn2)\n",
        "mylist=np.array(mylist,dtype=object)\n",
        "print(mylist[3])\n",
        "mylist = pad_sequences(mylist, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "mylist\n"
      ],
      "metadata": {
        "id": "2uWBXiJdaGsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels=[x.replace(\" \", \"\") for x in labels]"
      ],
      "metadata": {
        "id": "F8vowspmaHBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels[0]=0"
      ],
      "metadata": {
        "id": "fZk4zTQWcEZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(training_labels)):\n",
        "  if training_labels[x]=='0':\n",
        "    training_labels[x]=0\n",
        "  elif training_labels[x]=='1':\n",
        "    training_labels[x]=1\n",
        "  elif training_labels[x]=='2':\n",
        "    training_labels[x]=2\n",
        "  else:\n",
        "    training_labels[x]=int(training_labels[x])\n"
      ],
      "metadata": {
        "id": "Yqf58enUiaae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels=np.array(training_labels)"
      ],
      "metadata": {
        "id": "WREwNyfsqZEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_labels)"
      ],
      "metadata": {
        "id": "TbhOsQ8Bc4An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ys = tf.keras.utils.to_categorical(training_labels, num_classes=3)"
      ],
      "metadata": {
        "id": "JsF04OH7dBrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mylist))\n",
        "print(len(ys))"
      ],
      "metadata": {
        "id": "y9kYPMyE4lqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(LSTM(max_length-1, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(LSTM(max_length-1)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dLmYF9aHf-tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "history = model.fit(mylist, ys, epochs=num_epochs, verbose=2)\n"
      ],
      "metadata": {
        "id": "uwq_DUgUf_fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cate=[\"高興\",\"生氣\",\"發問\"]\n",
        "mylist=[]\n",
        "nn3=[]\n",
        "str3 = [\"保存期限還有多久?\",\"品質優良\"]\n",
        "str3=np.array(str3)\n",
        "for xx,yy in enumerate(str3):\n",
        "  str3[xx]=yy.replace(\"?\",\"\")\n",
        "\n",
        "str3=np.array(str3)\n",
        "\n",
        "for nn in str3:\n",
        "  nn2=getch(nn)\n",
        "  nn2 = tokenizer.texts_to_sequences(nn2)\n",
        "  nn2=np.array(nn2)\n",
        "  #padd = pad_sequences(str3, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "  nn2=nn2.flatten()\n",
        "  mylist.append(nn2)\n",
        "mylist=np.array(mylist)\n",
        "print(mylist)\n",
        "mylist = pad_sequences(mylist, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "\n",
        "predict=model.predict(mylist)\n",
        "#print(pred_classes.reshape(-1)[predicted])\n",
        "print(np.round(predict,2))\n",
        "print(np.argmax(predict,axis=1))\n",
        "result=[cate[x] for x in np.argmax(predict,axis=1)]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "a654QuHLq7g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preds(x):\n",
        "  mylist=[]\n",
        "  nn3=[]\n",
        "  str3=x.replace(\"?\",\"\")\n",
        "\n",
        "\n",
        "  nn2=getch(str3)\n",
        "  nn2 = tokenizer.texts_to_sequences(nn2)\n",
        "  nn2=np.array(nn2)\n",
        "    #padd = pad_sequences(str3, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "  nn2=nn2.flatten()\n",
        "  mylist.append(nn2)\n",
        "  mylist=np.array(mylist)\n",
        "  mylist = pad_sequences(mylist, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "  print(mylist)\n",
        "\n",
        "  predict=model.predict(mylist)\n",
        "  print(np.round(predict,2))\n",
        "  pred_index=np.argmax(predict,axis=1)\n",
        "  print(pred_index)\n",
        "  return cate[pred_index[0]]"
      ],
      "metadata": {
        "id": "w2yhwYQQyl92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preds(\"品質優良\"))"
      ],
      "metadata": {
        "id": "MLQxZZpzouDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}